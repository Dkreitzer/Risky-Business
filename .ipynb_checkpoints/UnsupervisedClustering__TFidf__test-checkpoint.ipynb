{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ref https://towardsdatascience.com/applying-machine-learning-to-classify-an-unsupervised-text-document-e7bb6265f52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "## This is for backend, gui support, but it doesnt do anything ##\n",
    "matplotlib.use('TkAgg') \n",
    "from matplotlib import figure, pyplot\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix, csr_matrix, lil_matrix\n",
    "from tqdm.auto import tqdm\n",
    "from operator import itemgetter\n",
    "import heapq\n",
    "import collections\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765be99f9a3b477e82275c8cb770e7b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### Rename RiskFactors > Data > Run  <for f in *\\ *; do mv \"$f\" \"${f// /_}\"; done> in bash, in Data ####\n",
    "##### This changes all \" \" characters to \"_\", needed for Path######\n",
    "##### See https://stackoverflow.com/a/18213120 #####\n",
    "\n",
    "\n",
    "NUMBER_OF_DOCS = 100\n",
    "doclist = []\n",
    "names = []\n",
    "pathlist = Path(\"Data_Small\").glob('**/*.txt')\n",
    "pathlist = Path(\"Data\").glob('**/*.txt')\n",
    "for path in tqdm(pathlist):\n",
    "    # because path is object not string\n",
    "    path_in_str = str(path)\n",
    "    name = path_in_str.split(\"\\\\\")[1].split(\".\")[0]\n",
    "    names.append(name)\n",
    "    #TODO SPLIT PATH TO COMPANY NAME, make Index\n",
    "    file = open(path, \"r\") \n",
    "    # print \"Output of Readlines after appending\"\n",
    "    text = file.readlines()\n",
    "#     print(text[:10])\n",
    "    doclist.append(text[0])\n",
    "if len(doclist) > NUMBER_OF_DOCS:\n",
    "    doclist = doclist[:NUMBER_OF_DOCS]\n",
    "    names = names[:NUMBER_OF_DOCS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 docs loaded\n",
      "\n",
      "['180_Degree_Capital_Corp_', '1_800_FLOWERS_COM_Inc_', '2U_Inc_', '3D_Systems_Corporation', '3M_Company', 'A10_Networks_Inc_', 'AAC_Holdings_Inc_', 'AAON_Inc_', 'AAR_Corp_', 'Abbott_Laboratories'] ...... ['Alaska_Air_Group_Inc_', 'Alaska_Communications_Systems_Group_Inc_', 'Albemarle_Corporation', 'Albireo_Pharma_Inc_', 'Alcentra_Capital_Corp_', 'Alcoa_Corporation', 'Alexander_39_s_Inc_', 'Alexander_Baldwin_Inc_', 'Alexandria_Real_Estate_Equities_Inc_', 'Alexion_Pharmaceuticals_Inc_']\n"
     ]
    }
   ],
   "source": [
    "print('%s docs loaded'% len(names))\n",
    "print()\n",
    "print(names[:10], '......',  names[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doclist[1]\n",
    "\n",
    "# len(doclist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(document):\n",
    "    split_text = [word.lower() for word in document.split(\".\")]\n",
    "#     TODO REPLACE WITH STOPWORDS??\n",
    "\n",
    "#     vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    return split_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_split(doclist):\n",
    "    big_split_text_list = []\n",
    "    big_split_name_list = []\n",
    "\n",
    "    for counter, document in tqdm(enumerate(doclist)):\n",
    "        name = names[counter]\n",
    "        split_text =  [word.lower() for word in document.split(\".\")]\n",
    "        big_split_name_list += [name for word in document.split(\".\")]\n",
    "        big_split_text_list += split_text\n",
    "#         if counter % 100 == 0:\n",
    "#             print(f'{counter} of {len(doclist)}')\n",
    "    return big_split_text_list, big_split_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(split_text_entry):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "    sparseMatrix = vectorizer.fit_transform(split_text_entry)\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit_transform\n",
    "    return sparseMatrix, vectorizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stops = ['\\\\t\\\\t\\\\t', \n",
    "        '\\\\t\\\\t\\\\', '\\\\t\\\\t\\\\t',\n",
    "        '<U+25CF>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec645dfafddf431b915cd88312eefa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wall time: 82.8 ms\n"
     ]
    }
   ],
   "source": [
    "# split_text = split_document(doclist[0])\n",
    "# sparseMatrix, vectorizer = vectorize_text(split_text)\n",
    "# for multiple files, uncomment below\n",
    "%time big_split_text_list, big_split_name_list = big_split(doclist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.99 s\n"
     ]
    }
   ],
   "source": [
    "%time sparseMatrix, vectorizer = vectorize_text(big_split_text_list)\n",
    "# I dont think we can get tqdm to work here #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['180_Degree_Capital_Corp_',\n",
       " '180_Degree_Capital_Corp_',\n",
       " '180_Degree_Capital_Corp_',\n",
       " '180_Degree_Capital_Corp_',\n",
       " '180_Degree_Capital_Corp_']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_split_name_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 5), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<72938x1607985 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3196298 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 210 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xanen\\Anaconda3\\envs\\Tensor\\lib\\site-packages\\matplotlib\\figure.py:445: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHEAAABICAYAAABx0mk3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAAhZJREFUeJzt2rENgDAQBEGM6L9l0wEWAYIVM/EFH69+zDk3AAAAAL5tf/sAAAAAANZEHAAAAIAAEQcAAAAgQMQBAAAACBBxAAAAAAJEHAAAAIAAEQcAAAAgQMQBAAAACBBxAAAAAAJEHAAAAICA4+Z+PnIFAAAAwL+N1cAnDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQIOIAAAAABIg4AAAAAAEiDgAAAECAiAMAAAAQcNzcj0euAAAAAOCSTxwAAACAABEHAAAAIEDEAQAAAAgQcQAAAAACRBwAAACAABEHAAAAIEDEAQAAAAgQcQAAAAACRBwAAACAgBN6VwSSytFoswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# This doesnt really work when passing many documents > 100?\n",
    "# https://stackoverflow.com/questions/22961541/python-matplotlib-plot-sparse-matrix-pattern\n",
    "def plot_coo_matrix(m):\n",
    "    if not isinstance(m, coo_matrix):\n",
    "        m = coo_matrix(m)\n",
    "    fig = pyplot.figure(figsize=(20, 10))\n",
    "    ax = fig.add_subplot(111, facecolor ='black')\n",
    "    ax.plot(m.col, m.row, 's', color='white', ms=3)\n",
    "    ax.set_xlim(0, m.shape[1])\n",
    "    ax.set_ylim(0, m.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "%time ax = plot_coo_matrix(sparseMatrix)\n",
    "ax.figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truek = PLEASE_FIND_ME\n",
    "def cluster_it(sparseMatrix, vectorizer):\n",
    "    truek = 25\n",
    "    model = KMeans(n_clusters=truek, init='k-means++', max_iter=100, n_init=1, random_state=42)\n",
    "    model.fit(sparseMatrix)\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    return terms, order_centroids, model, truek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "##THIS TAKES ~~ 1m per 100 documents\n",
    " %time terms, order_centroids, model, truek = cluster_it(sparseMatrix, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using joblib to pickle model\n",
    "model_time = datetime.now().strftime(\"%b%d-%I%M%p\")\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(model, f'outputs/s2s{model_time}.pkl')\n",
    "model = joblib.load(f'outputs/s2s{model_time}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = model.labels_.tolist()\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_it(big_split_text_list, big_split_name_list, clusters):\n",
    "    first_we_dict = {'company': big_split_name_list, 'split': big_split_text_list, 'cluster': clusters}\n",
    "    frame = pd.DataFrame(first_we_dict, index =clusters, columns = ['cluster', 'company', 'split'])   \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_df = dataframe_it(big_split_text_list, big_split_name_list, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>company</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>180_Degree_Capital_Corp_</td>\n",
       "      <td>[1] \"item 1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>180_Degree_Capital_Corp_</td>\n",
       "      <td>risk factors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>180_Degree_Capital_Corp_</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>180_Degree_Capital_Corp_</td>\n",
       "      <td>investing in our common stock involves signif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>180_Degree_Capital_Corp_</td>\n",
       "      <td>you should carefully consider the risks and u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                   company  \\\n",
       "0        0  180_Degree_Capital_Corp_   \n",
       "0        0  180_Degree_Capital_Corp_   \n",
       "0        0  180_Degree_Capital_Corp_   \n",
       "7        7  180_Degree_Capital_Corp_   \n",
       "7        7  180_Degree_Capital_Corp_   \n",
       "\n",
       "                                               split  \n",
       "0                                       [1] \"item 1a  \n",
       "0                                       risk factors  \n",
       "0                                                     \n",
       "7   investing in our common stock involves signif...  \n",
       "7   you should carefully consider the risks and u...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '00 42',\n",
       " '00 case',\n",
       " '00 case abr',\n",
       " '00 case abr loans',\n",
       " '00 case eurodollar',\n",
       " '00 case eurodollar loans',\n",
       " '00 case eurodollar loans september',\n",
       " '00 certain',\n",
       " '00 certain circumstances',\n",
       " '00 increase',\n",
       " '00 increase rates',\n",
       " '00 increase rates increase',\n",
       " '00 increase rates increase total',\n",
       " '00 june']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " business\n",
      " product\n",
      " products\n",
      " financial\n",
      " operations\n",
      "Cluster 1:\n",
      " 36\n",
      " million 36\n",
      " million\n",
      " µmol\n",
      " enforce certain patents related namenda\n",
      "Cluster 2:\n",
      " business operations suffer event failures\n",
      " suffer event failures\n",
      " operations suffer event failures\n",
      " event failures\n",
      " business operations suffer\n",
      "Cluster 3:\n",
      " pain\n",
      " dzuveo obtain label includes trauma\n",
      " label includes trauma related\n",
      " related pain addition\n",
      " study emergency department dzuveo\n",
      "Cluster 4:\n",
      " performance particular companies\n",
      " operating performance particular companies\n",
      " particular\n",
      " operating performance particular\n",
      " performance particular\n",
      "Cluster 5:\n",
      " 49\n",
      " 60 49\n",
      " 60\n",
      " µmol\n",
      " enforce certain\n",
      "Cluster 6:\n",
      " promotion\n",
      " improper promotion\n",
      " large civil criminal\n",
      " government levied\n",
      " fines companies alleged improper\n",
      "Cluster 7:\n",
      " stock\n",
      " common\n",
      " common stock\n",
      " price\n",
      " price common\n",
      "Cluster 8:\n",
      " based existing patents\n",
      " based existing patents patents\n",
      " based existing patents patents granted\n",
      " patents granted future\n",
      " existing patents patents granted future\n",
      "Cluster 9:\n",
      " commonly referred brexit\n",
      " referred brexit\n",
      " commonly referred\n",
      " brexit\n",
      " commonly\n",
      "Cluster 10:\n",
      " table contents\n",
      " table\n",
      " contents\n",
      " enforce agreements violated\n",
      " enforce agreements violated certainly possible\n",
      "Cluster 11:\n",
      " 53\n",
      " enforce agreements violated certainly possible\n",
      " enforce anti counterfeit\n",
      " enforce anti counterfeit drug\n",
      " enforce anti counterfeit drug pedigree\n",
      "Cluster 12:\n",
      " 31\n",
      " µmol\n",
      " enforce certain patents\n",
      " enforce agreements violated certainly possible\n",
      " enforce anti\n",
      "Cluster 13:\n",
      " active trading market common\n",
      " active trading market common stock\n",
      " active trading\n",
      " active trading market\n",
      " stock\n",
      "Cluster 14:\n",
      " required refunds\n",
      " refunds\n",
      " required\n",
      " µmol\n",
      " enforce antigen specific\n",
      "Cluster 15:\n",
      " 65\n",
      " million 65\n",
      " million\n",
      " µmol\n",
      " enforce certain patents related namenda\n",
      "Cluster 16:\n",
      " 30\n",
      " µmol\n",
      " enforce agreements violated certainly possible\n",
      " enforce anti counterfeit\n",
      " enforce anti counterfeit drug\n",
      "Cluster 17:\n",
      " advisors\n",
      " including scientific clinical advisors\n",
      " including scientific clinical\n",
      " consultants advisors including scientific clinical\n",
      " advisors including scientific clinical advisors\n",
      "Cluster 18:\n",
      " equity securities transactions prices\n",
      " securities transactions prices\n",
      " prices manner\n",
      " securities transactions prices manner\n",
      " securities equity securities transactions prices\n",
      "Cluster 19:\n",
      " marketing distribution infrastructure commercialize\n",
      " sales marketing distribution infrastructure commercialize\n",
      " establish sales marketing distribution infrastructure\n",
      " sales marketing distribution infrastructure\n",
      " distribution infrastructure commercialize\n",
      "Cluster 20:\n",
      " earnings fund\n",
      " future earnings fund\n",
      " future earnings fund development growth\n",
      " earnings fund development growth business\n",
      " fund development growth business\n",
      "Cluster 21:\n",
      " included annual report form 10\n",
      " included annual report form\n",
      " included annual report\n",
      " included annual\n",
      " information included\n",
      "Cluster 22:\n",
      " disclosure confidential information\n",
      " disclosure confidential\n",
      " confidential information\n",
      " disclosure\n",
      " confidential\n",
      "Cluster 23:\n",
      " applications\n",
      " pending applications supplements\n",
      " applications supplements\n",
      " approve pending applications supplements\n",
      " approve pending applications\n",
      "Cluster 24:\n",
      " agencies government contractors\n",
      " including government agencies\n",
      " parties including government\n",
      " government agencies government contractors\n",
      " parties including government agencies\n"
     ]
    }
   ],
   "source": [
    "nclosest_words_to_show = 5\n",
    "for i in range(truek):\n",
    " print('Cluster %d:' % i),\n",
    " for ind in order_centroids[i, :nclosest_words_to_show]:\n",
    "     print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prediction\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('Prediction')\n",
    "X = vectorizer.transform(['Market outlook on coffee looks promising'])\n",
    "predicted = model.predict(X)\n",
    "print(predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " business\n",
      "Cluster 1:\n",
      " 36\n",
      "Cluster 2:\n",
      " business operations suffer event failures\n",
      "Cluster 3:\n",
      " pain\n",
      "Cluster 4:\n",
      " performance particular companies\n",
      "Cluster 5:\n",
      " 49\n",
      "Cluster 6:\n",
      " promotion\n",
      "Cluster 7:\n",
      " stock\n",
      "Cluster 8:\n",
      " based existing patents\n",
      "Cluster 9:\n",
      " commonly referred brexit\n",
      "Cluster 10:\n",
      " table contents\n",
      "Cluster 11:\n",
      " 53\n",
      "Cluster 12:\n",
      " 31\n",
      "Cluster 13:\n",
      " active trading market common\n",
      "Cluster 14:\n",
      " required refunds\n",
      "Cluster 15:\n",
      " 65\n",
      "Cluster 16:\n",
      " 30\n",
      "Cluster 17:\n",
      " advisors\n",
      "Cluster 18:\n",
      " equity securities transactions prices\n",
      "Cluster 19:\n",
      " marketing distribution infrastructure commercialize\n",
      "Cluster 20:\n",
      " earnings fund\n",
      "Cluster 21:\n",
      " included annual report form 10\n",
      "Cluster 22:\n",
      " disclosure confidential information\n",
      "Cluster 23:\n",
      " applications\n",
      "Cluster 24:\n",
      " agencies government contractors\n"
     ]
    }
   ],
   "source": [
    "# HOW DO WE FIND the tag label?, 1Closest_word to cluster definitely isn't it\n",
    "for i in range(truek):\n",
    " print('Cluster %d:' % i),\n",
    " for ind in order_centroids[i, :1]:\n",
    "     print(' %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lil_matrix(sparseMatrix)[:, :1607985].shape[0]\n",
    "y = lil_matrix(sparseMatrix)[:72938, ].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<72938x1607985 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3196298 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-c50c5d37ce97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# The (n_clusters+1)*10 is for inserting blank space between silhouette\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# plots of individual clusters, to demarcate them clearly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Initialize the clusterer with n_clusters value and a random generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SILHOUETTE VALIDATION\n",
    "print(__doc__)\n",
    "\n",
    "# Generating the sample data from make_blobs\n",
    "# This particular setting has one distinct cluster and 3 clusters placed close\n",
    "# together.\n",
    "# X, y = sparseMatrix[0].shape[0], sparseMatrix[1].shape[1x1607985]\n",
    "\n",
    "range_n_clusters = range(truek)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TF-IDF analysis \n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "def get_wordset(doc_text):\n",
    "    words = doc_text.split(\" \")\n",
    "    wordSet = []\n",
    "    stopWordsSet =  set(stopwords.words('english'))\n",
    "\n",
    "#     print(stopWords)\n",
    "    for w in words:\n",
    "    #     wordSet.append(w)\n",
    "\n",
    "        if w not in stopWordsSet:\n",
    "            wordSet.append(w)\n",
    "    return wordSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordDict(wordSet):\n",
    "    wordDict = dict.fromkeys(wordSet, 0) \n",
    "    for word in wordSet:\n",
    "        wordDict[word]+=1\n",
    "\n",
    "    return wordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def computeTF(document_text):\n",
    "    wordset = get_wordset(document_text)\n",
    "    word_dict = get_wordDict(wordset)\n",
    "    tfDict = {}\n",
    "    word_count = len(wordset)\n",
    "    for word, count in word_dict.items():\n",
    "        tfDict[word] = count/float(word_count)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doclist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs = []\n",
    "for i  in range(len(doclist)):\n",
    "    print(i)\n",
    "    term_frequency = computeTF(doclist[i])\n",
    "    term_freqs.append(term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    big_wordset = []\n",
    "    for doc in docList:\n",
    "        big_wordset += get_wordset(doc)\n",
    "    big_wordDict = get_wordDict(big_wordset)\n",
    "        \n",
    "    idfDict = dict.fromkeys(big_wordDict.keys(), 0)\n",
    "  \n",
    "    for word, val in big_wordDict.items():\n",
    "        if val > 0:\n",
    "            idfDict[word] += 1\n",
    "\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF(doclist)\n",
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(term_frequency, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in term_frequency.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBow = computeTFIDF(term_frequency, idfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfBow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/4743035/obtaining-the-least-common-element-in-array\n",
    "def least_common_values(array, to_find=None):\n",
    "    counter = collections.Counter(array)\n",
    "    if to_find is None:\n",
    "        return sorted(counter.items(), key=itemgetter(1), reverse=False)\n",
    "    return heapq.nsmallest(to_find, counter.items(), key=itemgetter(1))\n",
    "least_common_values(tfidfBow, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(tfidfBow).most_common(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collections.Counter(term_frequency).most_common(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent Dirichlet Allocation\n",
    "# Ref http://brandonrose.org/clustering\n",
    "# LDA is a probabilistic topic model that assumes documents\n",
    "# are a mixture of topics and that each word in the document is attributable to the document's topics.\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "input_texts = doclist\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "import string\n",
    "def strip_proppers(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent) if word.islower()]\n",
    "    return \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in tokens]).strip()\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "#remove proper names0\n",
    "%time preprocess = [strip_proppers(doc) for doc in input_texts]\n",
    "\n",
    "#tokenize\n",
    "%time tokenized_text = [tokenize_and_stem(text) for text in preprocess]\n",
    "\n",
    "#remove stop words\n",
    "%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n",
    "# print(texts)\n",
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# print(dictionary)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "# dictionary.filter_extremes(no_below=1, no_above=0.8)  THIS FILTERS EVERYTHING OUT\n",
    "# print(dictionary)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# print(corpus)\n",
    "%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=10000, passes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=15)\n",
    "topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "topic_words = topics_matrix[:,1]\n",
    "for i in topic_words:\n",
    "    print([str(word[0]) for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing\n",
    "lsi = models.lsimodel.LsiModel(corpus, num_topics=5, id2word=dictionary, chunksize=1000)\n",
    "topics_matrix = lsi.show_topics(formatted=False, num_words=15)\n",
    "topics_matrix = np.array(topics_matrix)\n",
    "topic_words = topics_matrix[:,1]\n",
    "for i in topic_words:\n",
    "    print([str(word[0]) for word in i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(X[0])\n",
    "print()\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensor)",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
